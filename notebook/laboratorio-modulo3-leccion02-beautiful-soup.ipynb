{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/Hack-io-Data/Imagenes/blob/main/01-LogosHackio/logo_naranja@4x.png?raw=true\" alt=\"esquema\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "**Descripción:**\n",
    "\n",
    "SetMagic Productions es una empresa especializada en la provisión de servicios integrales para la realización de rodajes cinematográficos y audiovisuales. Nos dedicamos a facilitar tanto el atrezzo necesario para las producciones como los lugares idóneos para llevar a cabo los rodajes, ya sea en entornos al aire libre o en interiores.\n",
    "\n",
    "**Servicios Ofrecidos:**\n",
    "\n",
    "- **Atrezzo Creativo:** Contamos con un extenso catálogo de atrezzo que abarca desde accesorios hasta muebles y objetos temáticos para ambientar cualquier tipo de  escena.\n",
    "\n",
    "- **Locaciones Únicas:** Nuestra empresa ofrece una amplia selección de locaciones, que incluyen desde escenarios naturales como playas, bosques y montañas, hasta espacios interiores como estudios, casas históricas y edificios emblemáticos.\n",
    "- **Servicios de Producción:** Además de proporcionar atrezzo y locaciones, también ofrecemos servicios de producción audiovisual, incluyendo equipos de filmación, personal técnico y servicios de postproducción.\n",
    "\n",
    "**Herramientas y Tecnologías:**\n",
    "\n",
    "Para recopilar información sobre nuevas locaciones y tendencias en atrezzo, utilizamos herramientas de web scraping como Beautiful Soup y Selenium para extraer datos de sitios web relevantes y redes sociales especializadas en cine y producción audiovisual. También integramos APIs de plataformas de alquiler de locaciones y bases de datos de atrezzo para acceder a información actualizada y detallada.\n",
    "\n",
    "**Almacenamiento de Datos:** (A trabajar la próxima semana)\n",
    "\n",
    "La información recopilada mediante web scraping y APIs se almacenará tanto en una base de datos relacional SQL como en una base de datos no relacional MongoDB . Estas base de datos nos permite organizar eficientemente la información sobre locaciones, atrezzo, clientes y proyectos en curso, facilitando su acceso y gestión.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Nuestro objetivo principal es proporcionar a nuestros clientes una experiencia fluida y personalizada en la búsqueda y selección de locaciones y atrezzo para sus proyectos audiovisuales. Utilizando tecnologías avanzadas y una amplia red de contactos en la industria, nos esforzamos por ofrecer soluciones creativas y de alta calidad que satisfagan las necesidades específicas de cada producción.\n",
    "\n",
    "\n",
    "## Lab: Extracción de Información con Beautiful Soup\n",
    "\n",
    "En este laboratorio seguirás enriqueciendo la base de datos para ofrecer a tus clientes un servicio más completo y personalizado. Para lograr esto, extraerás información valiosa sobre objetos de atrezzo de una web especializada. Utilizarás técnicas de web scraping con la biblioteca **Beautiful Soup** para obtener y organizar los datos de manera eficiente.\n",
    "\n",
    "Trabajarás con la siguiente URL: [Atrezzo Vázquez](https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page=1). Esta página contiene una gran cantidad de objetos de atrezzo que pueden ser de interés para un proyecto de rodaje. Tu tarea será extraer información relevante de los productos listados en las primeras 100 páginas.\n",
    "\n",
    "1. **Navegación y Extracción de Múltiples Páginas**:\n",
    "\n",
    "   - La página web contiene varios elementos distribuidos en distintas páginas. Tu objetivo será iterar a través de las 100 primeras páginas y extraer la información deseada.\n",
    "\n",
    "   - Debes asegurarte de que tu código sea capaz de navegar automáticamente por estas páginas y extraer datos de manera continua.\n",
    "\n",
    "2. **Verificación del Código de Estado de la Respuesta**:\n",
    "\n",
    "   - Antes de extraer cualquier información, es fundamental verificar que la solicitud a la página web ha sido exitosa. Un código de estado 200 indica que la página se ha cargado correctamente.\n",
    "\n",
    "   - Si el código no es 200, debes imprimir un mensaje de error y detener la ejecución de la extracción para evitar problemas posteriores.\n",
    "\n",
    "3. **Extracción de Información Específica**:\n",
    "\n",
    "   De cada página, deberás extraer los siguientes detalles de los objetos de atrezzo:\n",
    "\n",
    "   - **Nombre del Objeto**: El nombre o identificador del objeto.\n",
    "\n",
    "   - **Categoría**: La categoría en la que se clasifica el objeto (ej.: mobiliario, decorado, utilería, etc.).\n",
    "\n",
    "   - **Sección**: La sección específica dentro de la categoría.\n",
    "\n",
    "   - **Descripción**: Una breve descripción del objeto que puede incluir detalles sobre su estilo, material o uso.\n",
    "\n",
    "   - **Dimensiones**: El tamaño del objeto en formato (largo x ancho x alto).\n",
    "\n",
    "   - **Enlace a la Imagen**: El link a la imagen del objeto, útil para tener una vista previa visual.\n",
    "\n",
    "4. **Organización de los Datos en un Diccionario**:\n",
    "\n",
    "   Una vez extraída la información, deberás organizarla en un diccionario con las siguientes claves:\n",
    "\n",
    "   - `\"nombre\"`: Nombres del objeto.\n",
    "\n",
    "   - `\"categoria\"`: Categoría a la que pertenece el objeto.\n",
    "\n",
    "   - `\"seccion\"`: Sección específica dentro de la categoría.\n",
    "\n",
    "   - `\"descripcion\"`: Breve descripción del objeto.\n",
    "\n",
    "   - `\"dimensiones\"`: Dimensiones del objeto en el formato adecuado.\n",
    "\n",
    "   - `\"imagen\"`: URL de la imagen del objeto.\n",
    "\n",
    "5. **Almacenamiento de la Información en un DataFrame**:\n",
    "\n",
    "   Una vez que tengas toda la información organizada en diccionarios, el siguiente paso es convertirla en un DataFrame de Pandas. Este DataFrame te permitirá manipular y analizar los datos con facilidad. Tu DataFrame debería verse similar al ejemplo proporcionado, donde cada fila representa un objeto diferente y cada columna contiene la información extraída:\n",
    "\n",
    "![Dataframe](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/BS/df_atrezzo.png?raw=true)\n",
    "\n",
    "\n",
    "6.  Consideraciones Adicionales:\n",
    "\n",
    "- Asegúrate de manejar posibles errores o excepciones durante el scraping, como tiempos de espera agotados, páginas inaccesibles o datos faltantes.\n",
    "\n",
    "- Recuerda que el scraping debe hacerse de manera respetuosa, evitando sobrecargar el servidor de la página web (puedes usar pausas entre solicitudes).\n",
    "\n",
    "- Finalmente, asegúrate de almacenar el DataFrame en un archivo CSV para que puedas reutilizar esta información en análisis futuros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Navegación y Extracción de Múltiples Páginas**:\n",
    "\n",
    "   - La página web contiene varios elementos distribuidos en distintas páginas. Tu objetivo será iterar a través de las 100 primeras páginas y extraer la información deseada.\n",
    "\n",
    "   - Debes asegurarte de que tu código sea capaz de navegar automáticamente por estas páginas y extraer datos de manera continua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_atrezzo = \"https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenido = requests.get(url_atrezzo)\n",
    "contenido.status_code\n",
    "sopa_atrezzo = BeautifulSoup(contenido.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos los nombres y las categorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una llamada a la clase padre de nombre y categoría.\n",
    "lista_producto = sopa_atrezzo.findAll(\"div\", {\"class\":\"product-slide-entry shift-image\"})\n",
    "\n",
    "lista_nombres = []\n",
    "lista_categorias = []\n",
    "\n",
    "for producto in lista_producto:\n",
    "    # Si no existe la clase title apendeamos un NaN y si no el nombre\n",
    "    try:\n",
    "        nombre = producto.findAll(\"a\", {\"class\":\"title\"})[0].getText()\n",
    "        lista_nombres.append(nombre)\n",
    "    except:\n",
    "        lista_nombres.append(np.nan)\n",
    "\n",
    "    # Si no existe la clase tag apendeamos un NaN y si no la categoria\n",
    "    try:\n",
    "        tag = producto.findAll(\"a\", {\"class\":\"tag\"})[0].getText()\n",
    "        lista_categorias.append(tag)\n",
    "    except:\n",
    "        lista_categorias.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(lista_nombres))\n",
    "print(len(lista_categorias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accedemos a las secciones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Arabe\\xa0\\xa0', nan, 'Dormitorio\\xa0\\xa0']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_seccion_html = sopa_atrezzo.findAll(\"div\", {\"class\":\"cat-sec-box\"})\n",
    "lista_secciones = [seccion.getText() if len(seccion.getText())>1 else np.nan for seccion in lista_seccion_html]\n",
    "print(len(lista_secciones))\n",
    "lista_secciones[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Buscamos las descripciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_descripcion_html_1 = sopa_atrezzo.findAll(\"div\", {\"class\":\"article-container style-1\"})\n",
    "len(lista_descripcion_html_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos accedido a la lista y comprobado que los primeros 48 elementos tiene un parametro `<p>` donde viene la descripción. Los elementos restantes no presentan esta etiqueta ya que representan los botones de la página y los enlaces a los que llevan dichos botones. Tras investigar comprobamos que los 48 primeros elementos son siempre las descripciones, por lo que cortamos la lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alfombra persa marrón (tiene unas manchas que no se quitan).',\n",
       " 'Vitrina con abanico',\n",
       " 'Alfombrín de cama círculos azules']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_descripcion_html = lista_descripcion_html_1[:48]\n",
    "lista_descripcion = [descripcion.findAll(\"p\")[0].getText() if len(descripcion.findAll(\"p\")[0].getText())>1 else np.nan for descripcion in lista_descripcion_html]\n",
    "lista_descripcion[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_descripcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sacamos las dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n460x340x1 (cm)\\n', '\\n100x10x60 (cm)\\n', '\\n150x72x1 (cm)\\n']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_dimensiones_html = sopa_atrezzo.findAll(\"div\", {\"class\":\"price\"})\n",
    "lista_dimensiones = [dimension.getText() for dimension in lista_dimensiones_html]\n",
    "lista_dimensiones[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sacamos el link de las imagenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_links_html = sopa_atrezzo.findAll(\"div\", {\"class\":\"product-image\"})\n",
    "lista_links=[]\n",
    "for link in lista_links_html:\n",
    "    try:\n",
    "        l = link.findAll(\"img\")[0].get(\"src\")\n",
    "        lista_links.append(l)\n",
    "    except:\n",
    "        lista_links.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a juntarlo todo en un for que recorra las 100 páginas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "limit = 48\n",
    "flag = 0\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(1,101)):\n",
    "    url_atrezzo = f\"https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit={limit}&page={i}\"\n",
    "    contenido = requests.get(url_atrezzo)\n",
    "    if (contenido.status_code != 200):\n",
    "        flag = 1\n",
    "        break\n",
    "    sopa_atrezzo = BeautifulSoup(contenido.content, \"html.parser\")\n",
    "\n",
    "    # --------- SACAMOS NOMBRES Y CATEGORÍA-----------------------------\n",
    "    lista_producto = sopa_atrezzo.findAll(\"div\", {\"class\":\"product-slide-entry shift-image\"})\n",
    "\n",
    "    lista_nombres = []\n",
    "    lista_categorias = []\n",
    "\n",
    "    for producto in lista_producto:\n",
    "        # Si no existe la clase title apendeamos un NaN y si no el nombre\n",
    "        try:\n",
    "            nombre = producto.findAll(\"a\", {\"class\":\"title\"})[0].getText()\n",
    "            lista_nombres.append(nombre)\n",
    "        except:\n",
    "            lista_nombres.append(np.nan)\n",
    "\n",
    "        # Si no existe la clase tag apendeamos un NaN y si no la categoria\n",
    "        try:\n",
    "            tag = producto.findAll(\"a\", {\"class\":\"tag\"})[0].getText()\n",
    "            lista_categorias.append(tag)\n",
    "        except:\n",
    "            lista_categorias.append(np.nan)\n",
    "\n",
    "\n",
    "    # -----------------SACAMOS LAS SECCIONES ---------------------------\n",
    "    lista_seccion_html = sopa_atrezzo.findAll(\"div\", {\"class\":\"cat-sec-box\"})\n",
    "    lista_secciones = [seccion.getText() if len(seccion.getText())>1 else np.nan for seccion in lista_seccion_html]\n",
    "   \n",
    "\n",
    "   # ---------------- SACAMOS LAS DESCRIPCIONES -----------------------\n",
    "    lista_descripcion_html_1 = sopa_atrezzo.findAll(\"div\", {\"class\":\"article-container style-1\"})\n",
    "    lista_descripcion_html = lista_descripcion_html_1[:limit]\n",
    "    lista_descripcion = [descripcion.findAll(\"p\")[0].getText() if len(descripcion.findAll(\"p\")[0].getText())>1 else np.nan for descripcion in lista_descripcion_html]\n",
    "\n",
    "    # ------------- SACAMOS LAS DIMENSIONES ------------------------------\n",
    "    lista_dimensiones_html = sopa_atrezzo.findAll(\"div\", {\"class\":\"price\"})\n",
    "    lista_dimensiones = [dimension.getText() for dimension in lista_dimensiones_html]\n",
    "\n",
    "    # ---------------- SACAMOS LOS LINKS ----------------------------------\n",
    "    lista_links_html = sopa_atrezzo.findAll(\"div\", {\"class\":\"product-image\"})\n",
    "    lista_links=[]\n",
    "    for link in lista_links_html:\n",
    "        try:\n",
    "            l = link.findAll(\"img\")[0].get(\"src\")\n",
    "            lista_links.append(\"https://atrezzovazquez.es/\"+l)\n",
    "        except:\n",
    "            lista_links.append(np.nan)\n",
    "\n",
    "    diccionario_pagina = {\n",
    "        \"nombre\":lista_nombres,\n",
    "        \"categoría\":lista_categorias,\n",
    "        \"seccion\":lista_secciones,\n",
    "        \"descripción\":lista_descripcion,\n",
    "        \"dimensiones\":lista_dimensiones,\n",
    "        \"imagen\":lista_links\n",
    "    }\n",
    "\n",
    "    df_pag = pd.DataFrame(diccionario_pagina)\n",
    "    df = pd.concat([df,df_pag])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dimensiones\"] = df[\"dimensiones\"].str.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 6)\n",
      "0\n",
      "Index(['nombre', 'categoría', 'seccion', 'descripción', 'dimensiones',\n",
      "       'imagen'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.duplicated().sum())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como todo esta correcto guardamos el dataframe\n",
    "df.to_csv(\"../datos/df_atrezzo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
